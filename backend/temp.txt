Python Backend Architecture for Voice Translation Application

The backend should be built using FastAPI for its excellent async support and WebSocket capabilities. Here's how I would structure it:
Core Components

    WebSocket Server: Handles real-time bidirectional communication with clients
    Room Manager: Creates and manages translation rooms
    Audio Processing Pipeline: Processes incoming audio streams through STT → Translation → TTS
    Model Manager: Handles different AI models (local and API-based)

Implementation Details
Server Setup (main.py)

import uuid
import asyncio
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from typing import Dict, List, Optional

from room_manager import RoomManager
from audio_processor import AudioProcessor
from model_manager import ModelManager

app = FastAPI()

# Add CORS to allow connections from your frontend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize components
model_manager = ModelManager()
audio_processor = AudioProcessor(model_manager)
room_manager = RoomManager(audio_processor)

@app.get("/")
async def root():
    return {"message": "Voice Translation API is running"}

@app.websocket("/ws/{room_id}")
async def websocket_endpoint(websocket: WebSocket, room_id: str, target_lang: str = "es"):
    """
    Handle WebSocket connections for a specific room
    """
    # Accept the connection
    await websocket.accept()
    
    # Generate a unique participant ID
    participant_id = str(uuid.uuid4())
    
    try:
        # Add the participant to the room
        await room_manager.add_participant(room_id, participant_id, websocket, target_lang)
        
        # Event loop to handle incoming messages
        while True:
            # Receive binary audio data
            audio_data = await websocket.receive_bytes()
            
            # Process the audio in the room
            await room_manager.process_audio(room_id, participant_id, audio_data)
            
    except WebSocketDisconnect:
        # Remove participant when disconnected
        await room_manager.remove_participant(room_id, participant_id)
    except Exception as e:
        print(f"Error: {e}")
        await room_manager.remove_participant(room_id, participant_id)

@app.get("/create-room")
async def create_room():
    """
    Create a new room and return its ID
    """
    room_id = await room_manager.create_room()
    return {"room_id": room_id}

@app.get("/rooms/{room_id}/participants")
async def get_room_participants(room_id: str):
    """
    Get the number of participants in a room
    """
    participants = await room_manager.get_participants(room_id)
    return {"participants": len(participants)}

# Run with: uvicorn main:app --host 0.0.0.0 --port 8000

Room Manager (room_manager.py)

import uuid
import asyncio
from typing import Dict, List, Set
from fastapi import WebSocket

class RoomManager:
    def __init__(self, audio_processor):
        self.rooms: Dict[str, Dict] = {}
        self.audio_processor = audio_processor
    
    async def create_room(self) -> str:
        """Create a new room with a unique ID"""
        room_id = str(uuid.uuid4())
        self.rooms[room_id] = {
            "participants": {},
            "audio_queue": asyncio.Queue()
        }
        # Start the room's audio processing task
        asyncio.create_task(self._process_room_audio(room_id))
        return room_id
    
    async def add_participant(self, room_id: str, participant_id: str, websocket: WebSocket, target_lang: str):
        """Add a participant to a room"""
        # Create room if it doesn't exist
        if room_id not in self.rooms:
            await self.create_room()
            
        # Add participant
        self.rooms[room_id]["participants"][participant_id] = {
            "websocket": websocket,
            "target_lang": target_lang
        }
        
        # Notify all participants about the update
        await self._broadcast_participants_update(room_id)
    
    async def remove_participant(self, room_id: str, participant_id: str):
        """Remove a participant from a room"""
        if room_id in self.rooms and participant_id in self.rooms[room_id]["participants"]:
            del self.rooms[room_id]["participants"][participant_id]
            
            # Remove room if empty
            if not self.rooms[room_id]["participants"]:
                del self.rooms[room_id]
            else:
                # Notify remaining participants
                await self._broadcast_participants_update(room_id)
    
    async def get_participants(self, room_id: str) -> List[str]:
        """Get all participants in a room"""
        if room_id in self.rooms:
            return list(self.rooms[room_id]["participants"].keys())
        return []
    
    async def process_audio(self, room_id: str, sender_id: str, audio_data: bytes):
        """Process audio from a participant and queue it for translation"""
        if room_id in self.rooms:
            # Add to the room's audio queue
            await self.rooms[room_id]["audio_queue"].put({
                "sender_id": sender_id,
                "audio_data": audio_data
            })
    
    async def _process_room_audio(self, room_id: str):
        """Process audio in the room's queue continuously"""
        while room_id in self.rooms:
            try:
                # Get the next audio chunk
                audio_item = await self.rooms[room_id]["audio_queue"].get()
                sender_id = audio_item["sender_id"]
                audio_data = audio_item["audio_data"]
                
                # Skip processing if room no longer exists
                if room_id not in self.rooms:
                    break
                
                # Process for each participant with their target language
                for participant_id, participant_info in self.rooms[room_id]["participants"].items():
                    # Skip sending back to original sender
                    if participant_id == sender_id:
                        continue
                    
                    target_lang = participant_info["target_lang"]
                    websocket = participant_info["websocket"]
                    
                    # Process audio with translation pipeline
                    translated_audio = await self.audio_processor.process_audio(
                        audio_data,
                        target_lang
                    )
                    
                    # Send translated audio to the participant
                    await websocket.send_bytes(translated_audio)
                
            except Exception as e:
                print(f"Error processing audio: {e}")
                await asyncio.sleep(0.1)
    
    async def _broadcast_participants_update(self, room_id: str):
        """Broadcast the number of participants to everyone in the room"""
        if room_id in self.rooms:
            participant_count = len(self.rooms[room_id]["participants"])
            for participant_info in self.rooms[room_id]["participants"].values():
                try:
                    await participant_info["websocket"].send_json({
                        "type": "participants_update",
                        "count": participant_count
                    })
                except Exception as e:
                    print(f"Error broadcasting update: {e}")

Audio Processor (audio_processor.py)

import asyncio
import io
from typing import Optional, Dict
import numpy as np

class AudioProcessor:
    def __init__(self, model_manager):
        self.model_manager = model_manager
        # Audio settings
        self.sample_rate = 16000
        self.chunk_size = 4096
    
    async def process_audio(self, audio_data: bytes, target_lang: str) -> bytes:
        """
        Process audio through the STT → Translation → TTS pipeline
        """
        # Process in a separate thread to avoid blocking
        return await asyncio.to_thread(self._process_audio_sync, audio_data, target_lang)
    
    def _process_audio_sync(self, audio_data: bytes, target_lang: str) -> bytes:
        """
        Synchronous version of the audio processing pipeline
        """
        try:
            # Convert bytes to numpy array for processing
            audio_np = np.frombuffer(audio_data, dtype=np.int16)
            
            # 1. Speech-to-Text
            transcript = self.model_manager.speech_to_text(audio_np)
            
            # Skip empty transcripts
            if not transcript or transcript.isspace():
                return b""
            
            # 2. Translation
            translated_text = self.model_manager.translate_text(transcript, target_lang)
            
            # 3. Text-to-Speech
            translated_audio = self.model_manager.text_to_speech(translated_text, target_lang)
            
            # Return the processed audio as bytes
            return translated_audio
            
        except Exception as e:
            print(f"Error in audio processing: {e}")
            return b""

Model Manager (model_manager.py)

import os
import numpy as np
from typing import Optional, Dict, List
import io

class ModelManager:
    def __init__(self):
        # Determine which mode to use based on environment variable
        self.use_local_models = os.getenv("USE_LOCAL_MODELS", "false").lower() == "true"
        
        # Initialize models based on mode
        if self.use_local_models:
            self._init_local_models()
        else:
            self._init_api_clients()
    
    def _init_local_models(self):
        """Initialize local AI models"""
        try:
            # Speech-to-Text (Whisper or Vosk)
            import whisper
            self.stt_model = whisper.load_model("small")
            
            # Translation (e.g., MarianMT from HuggingFace)
            from transformers import MarianMTModel, MarianTokenizer
            self.translation_models = {}
            
            # Text-to-Speech (e.g., Coqui TTS)
            import TTS
            from TTS.utils.synthesizer import Synthesizer
            self.tts_model = Synthesizer(
                tts_checkpoint="path/to/model.pth",
                tts_config_path="path/to/config.json",
                vocoder_checkpoint="path/to/vocoder.pth",
                vocoder_config="path/to/vocoder_config.json"
            )
            
            print("Local models loaded successfully")
            
        except Exception as e:
            print(f"Error loading local models: {e}")
            # Fallback to API mode
            self.use_local_models = False
            self._init_api_clients()
    
    def _init_api_clients(self):
        """Initialize API clients for cloud services"""
        # OpenAI API (for Whisper API)
        self.openai_api_key = os.getenv("OPENAI_API_KEY", "")
        
        # DeepL API (for translation)
        self.deepl_api_key = os.getenv("DEEPL_API_KEY", "")
        
        # ElevenLabs API (for TTS)
        self.elevenlabs_api_key = os.getenv("ELEVENLABS_API_KEY", "")
        
        print("API clients initialized")
    
    def speech_to_text(self, audio_data: np.ndarray) -> str:
        """Convert speech to text using the appropriate model"""
        if self.use_local_models:
            # Use local Whisper model
            result = self.stt_model.transcribe(audio_data)
            return result["text"]
        else:
            # Use OpenAI Whisper API
            import openai
            openai.api_key = self.openai_api_key
            
            # Save audio to temporary file
            temp_file = io.BytesIO()
            np.save(temp_file, audio_data)
            temp_file.seek(0)
            
            try:
                response = openai.Audio.transcribe("whisper-1", temp_file)
                return response["text"]
            except Exception as e:
                print(f"OpenAI API error: {e}")
                return ""
    
    def translate_text(self, text: str, target_lang: str) -> str:
        """Translate text to the target language"""
        if self.use_local_models:
            # Use local translation model
            # Would need to load the specific model for the language pair
            # This is simplified for brevity
            return f"Translated: {text}"
        else:
            # Use DeepL API
            import requests
            
            try:
                response = requests.post(
                    "https://api.deepl.com/v2/translate",
                    data={
                        "auth_key": self.deepl_api_key,
                        "text": text,
                        "target_lang": target_lang.upper()
                    }
                )
                result = response.json()
                return result["translations"][0]["text"]
            except Exception as e:
                print(f"DeepL API error: {e}")
                return text
    
    def text_to_speech(self, text: str, target_lang: str) -> bytes:
        """Convert text to speech in the target language"""
        if self.use_local_models:
            # Use local TTS model
            wavs = self.tts_model.tts(text)
            
            # Convert to bytes
            buffer = io.BytesIO()
            import soundfile as sf
            sf.write(buffer, wavs, 22050, format='wav')
            buffer.seek(0)
            return buffer.read()
        else:
            # Use ElevenLabs API
            import requests
            
            try:
                voice_id = "pNInz6obpgDQGcFmaJgB"  # Example voice ID
                response = requests.post(
                    f"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}",
                    headers={
                        "Content-Type": "application/json",
                        "xi-api-key": self.elevenlabs_api_key
                    },
                    json={
                        "text": text,
                        "model_id": "eleven_monolingual_v1",
                        "voice_settings": {
                            "stability": 0.5,
                            "similarity_boost": 0.5
                        }
                    }
                )
                
                if response.status_code == 200:
                    return response.content
                else:
                    print(f"ElevenLabs API error: {response.text}")
                    return b""
            except Exception as e:
                print(f"TTS API error: {e}")
                return b""

Requirements.txt

fastapi==0.95.1
uvicorn==0.22.0
numpy==1.24.3
websockets==11.0.3
python-multipart==0.0.6
requests==2.31.0

# For local models (optional)
# whisper==1.0.0
# transformers==4.30.0
# torch==2.0.1
# TTS==0.14.0
# soundfile==0.12.1

Deployment Instructions

    Create a virtual environment:

    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate

    Install dependencies:

    pip install -r requirements.txt

    Run the server:

    uvicorn main:app --host 0.0.0.0 --port 8000 --reload

    Set environment variables:
        For API mode: Set OPENAI_API_KEY, DEEPL_API_KEY, and ELEVENLABS_API_KEY
        For local models: Set USE_LOCAL_MODELS=true and ensure models are downloaded

Frontend Integration

To connect your React frontend to this backend:

    Update the useRoomConnection hook to connect to the WebSocket server:

// In useRoomConnection.js
const wsBase = "ws://localhost:8000/ws";

const connectWebSocket = (roomId, targetLanguage) => {
  const ws = new WebSocket(`${wsBase}/${roomId}?target_lang=${targetLanguage}`);
  
  ws.onopen = () => {
    // Handle connection established
  };
  
  ws.onmessage = (event) => {
    if (event.data instanceof Blob) {
      // Handle audio data
      const audioBlob = event.data;
      // Play the audio or pass to callback
    } else {
      // Handle JSON messages (participant updates, etc.)
      const data = JSON.parse(event.data);
      if (data.type === "participants_update") {
        // Update participant count
      }
    }
  };
  
  // Return the WebSocket instance
  return ws;
};
